


# 招银

## Sxxxxx基础框架

功能:
- 数据库连接池 ???
    - 代理模式: 通过代理模式创建DataSource实例, 然后代理该实例, 并将代理层注册为spring bean
    - 通过spring的AutoConfiguration机制, 解耦连接池与具体的ORM框架(mybatis)
    - 在代理层中加入before/after等方法, 在其中加入监控等业务无关逻辑
    - 在代理层的getConnection()方法中实现连接池逻辑
    - 相关问题:
        - JDBC驱动与SPI机制
- SFTP: 数据库<->文件
    - 重点: 
        - 多线程导出实现
            - 读写异步&解耦
        - 流式(迭代器模式) & 懒加载
- 客户端: HttpClient, TcpClient
    - 客户端连接池: 实现 ???
- 服务端: TcpServer
    - TCP问题
        - 粘包&延时: NO_DELAY
        - SO_LINGER: 0
    - 线程池



## 服务注册中心

服务注册中心
- 功能:
    - 服务注册: 上下线状态变更, 心跳状态变更
        - 鉴权: 带超时token
        - 新实例上下线时, 同时同步到实例缓存、心跳缓存、数据库, 并通过kafka同步其它机房
        - 心跳缓存: 
            - 接收到的心跳先放到缓存A中
            - 定时写入心跳缓存(15s): A<->B切换, 然后将心跳批量写入数据库
        - 实例状态缓存:
            - 单线程, 定时从DB读取全量实例状态到一个全新map, 然后全量替换
            - 新实例上下线时, 同时同步到实例缓存、心跳缓存、数据库、kafka
    - 分库路由
    - 跨机房同步: 
        - 实时更新: kafka同步
        - 批量更新: 数据库同步
    - 超时探测
        - 单线程定时扫描数据库中实例状态
- 关键问题:
    - 分布式问题:
        - 高可用: 多实例; 客户端熔断; 对数据库故障的鲁棒性
            - 部分可用: 深圳&上海分中心
        - 一致性: 最终一致性
            - kafka同步
            - 数据库同步(兜底)
        - 时钟同步: 校准请求.
            - 衰减均值: dt = dt' * (1 - alpha) +  [(s1 + r1) / 2 - s2] * alpha


## 微服务客户端

微服务客户端:
- 功能:
    - (服务注册)上下线, 心跳
        - 上线: 发送上线消息给同机房中心, 由kafka同步至另一个机房
        - 下线: 
    - 服务订阅与刷新
        - 多线程订阅: 
    - 分片路由
        - 单库、列表分片、范围分片、随机分片
    - 负载均衡 & 熔断
        - RoundRobin
        - Random
        - 响应时间动态加权
- 关键问题:
    - 高并发:
        - 高并发订阅保证线程安全
        - 无锁
        - CopyOnWrite
    - 服务中心宕机
        - fallback机制: 客户端缓存 & 本地缓存
- 设计上的亮点:
    - 拦截器模式
    - 实现了依赖注入
  

# 字节

数据中台
- 涉及技术: mysql, ES, clickhouse, 设计模式等 
- 项目简介: 提供各类广告数据的查询和报表导出功能, 支撑TikTok国际化广告、商务中心等平台, 向全球用户提供服务
      - 基于CQRS架构模式设计, 支持对接ES、clickhouse等数据源, 适应不同场景
    - 支持通用的指标查询(包括**筛选**、**聚合**、**分页**、**时间段对比**等功能), 以及报表离线导出功能
    - 支持对接ES、clickhouse等多种数据库, 支持不同查询场景
    - 基于CQRS架构模式设计, 支持查询视图实时+定时更新
- 负责内容:
    - 中台基础功能建设:
        - 查询引擎支持**逻辑表达式**引擎, 运行时解释用户指令生成SQL、DSL
        - 查询引擎lowcode改造, 减少非业务代码量, 并为后续实现可视化编程奠定基础
        - 基于CountDownLatch实现并发的拓扑排序, 实现中台层查询任务自动编排
        - 千万量级用户数据回溯, 保证**可用性**、**一致性**
        - 其它优化: 数据对比查询优化; go项目通用并发工具开发等
    - 主导 & 推动广告平台业务需求开发:
        - 评论洞察(Comment insight)首页报表: 通过丰富的评论数据(情感占比、情感趋势、行业对比、热门关键词)反馈广告效果, 有助于广告主改进广告和产品策略. 访问量(UV)稳定在每周1000+, 并获得Amazon、Good American等大客的正面直接反馈
        - 商务中心首页重构

- 相关知识点
    - ES: wildcard & keyword, 分词
    - go: chan & select
    - clickhouse: 按列存储
    - mysql: 索引


### 项目关键问题
#### 索引部署方案与刷库流程


部署方案
- 主、备索引, 正常情况下访问主索引, 主索引有问题或者需要回溯时, 将线上流量切至备索引
- 同步:
  - 实时: data_index 监听DB表binlog, 出现更新时, 实时查询 data_read 服务, 获取最新的数据
  - 定时: 通过cronjob定时扫表, 查询最近更新(modify_time更改)的记录
    - 监听的DB都有读/写两个版本, cronjob都尽量读取读表
      - 对于量级比较小的表, 直接全表扫描, 无须辅助索引.
      - 对于量级较大, 但是更新不太频繁的表, 对modify_time列创建索引, 通过较小的update代价换取较小的扫描成本
      - 对于量级较大, 但是更新较频繁的表, 需要要求业务方这边冗余一个log表(只建不删), 定时扫log表. 通过存储成本换取扫描成本
- 版本号: 基于DB表中modify_time计算version, 在写入logstash时传入, 由logstash判断version, 决定是否写入
- 部分索引会进行分区(由ES支持)


刷库
1. 回溯备索引
   1. 保持 data_index -> mq -> logstash -> ES实时更新链路, 保证最近的更新落库
   2. 将增量更新通过脚本写入 mq. 脚本产生的更新消息与自然产生的更新消息按顺序写入mq
   - version保证了ES更新顺序与数据库一致
2. 线上流量切换至备索引, 开始reindex主索引
   1. 暂停主索引的 mq -> logstash, 线上更新消息堆积在mq中
   2. 执行 ES 的 reindex, 将备索引复制到主索引
      - 由于备索引在实时更新, 在复制完后, 主索引必然落后于备索引
   3. 将主索引消费mq的offset 重置到reindex. 直到追上备索引 mq
      - mq 暂存了reindex 期间的更新消息, 并且带有版本号及完整信息
      - 这一阶段中新产生的更新消息追加在mq的后面, 保证了更新的顺序
3. 将流量切回主索引


关键点
- 一致性 & 幂等性:
  - data_read实时计算数据 (保持相同计算逻辑的前提下, 将实时查询服务实例与离线查询服务实例区分开)
  - 该项目适用于低计算量、高实时性的查询场景. 涉及大计算量的、维度变换的指标(如uv指标), 通常由OLAP平台来实现离线计算与同步, 提供有限的实时性保证(小时级、天级)
- 容错性:
  - 回溯过程可重试: 如果备索引损坏, 则把物理索引删除掉, 再重新拷贝一份主索引, 重新回溯
  - reindex过程可重试: 如果reindex失败, 则把物理索引删除掉, 重新执行整个reindex流程
- 可用性:
  - 有主、备两个索引, 在回溯期间始终有一个索引可以提供服务
- 减轻对下游的负载:
  - 在需求开发初期就推动下游优化:
    - 支持批量范围接口, 既减少了IO次数, 也利用了下游数据库索引的优势
    - 下游来源数据库分片, ES跟随DB的分片方案
- 级联更新:
  - 该系统支持的场景大多为星型模型(1事实表 + n 维度表), 在一个ES索引中保存多张表的信息, 以事实表的主键为索引主键.
  - 理想情况下, data_index 会定时监听所有关联表的binlog, 并且定期扫描. 但实际开发时, 会评估表的更新频率、数据量级, 决定是否需要监听、定时扫描


关联知识点
- CAP
- zookeeper